{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPD Manifold: Robust Covariance Matrix Estimation\n",
    "\n",
    "This notebook demonstrates robust covariance matrix estimation on the Symmetric Positive Definite (SPD) manifold. We compare standard maximum likelihood estimation with manifold-based robust estimation that is resilient to outliers.\n",
    "\n",
    "## Applications\n",
    "- **Computer vision**: Robust covariance descriptors for image classification\n",
    "- **Finance**: Portfolio optimization with heavy-tailed return distributions  \n",
    "- **Signal processing**: Covariance matrix estimation in the presence of noise\n",
    "- **Machine learning**: Robust Gaussian mixture model parameter estimation\n",
    "\n",
    "## Mathematical Background\n",
    "The SPD manifold P(n) = {X ∈ R^{n×n} : X = X^T, X ≻ 0} equipped with the affine-invariant Riemannian metric provides a natural framework for covariance matrix estimation that respects the geometric structure of positive definite matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import riemannax as rx\n",
    "\n",
    "# Enable 64-bit precision for better numerical stability\n",
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation with Outliers\n",
    "\n",
    "We generate multivariate data with a known covariance structure and add outliers to test robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multivariate_data_with_outliers(key, n_samples=200, n_features=4, outlier_ratio=0.1):\n",
    "    \"\"\"Generate multivariate data with outliers for robust estimation testing.\"\"\"\n",
    "    keys = jax.random.split(key, 4)\n",
    "\n",
    "    # True covariance structure with correlation\n",
    "    true_cov = jnp.array([\n",
    "        [1.0, 0.5, 0.2, 0.1],\n",
    "        [0.5, 1.0, 0.3, 0.0], \n",
    "        [0.2, 0.3, 1.0, 0.4], \n",
    "        [0.1, 0.0, 0.4, 1.0]\n",
    "    ])\n",
    "\n",
    "    # Generate normal samples\n",
    "    n_clean = int(n_samples * (1 - outlier_ratio))\n",
    "    clean_data = jax.random.multivariate_normal(keys[0], jnp.zeros(n_features), true_cov, (n_clean,))\n",
    "\n",
    "    # Generate outlier samples (heavy-tailed distribution)\n",
    "    n_outliers = n_samples - n_clean\n",
    "    outlier_scale = 3.0  # Scale factor for outliers\n",
    "    outlier_data = outlier_scale * jax.random.multivariate_normal(\n",
    "        keys[1], jnp.zeros(n_features), jnp.eye(n_features), (n_outliers,)\n",
    "    )\n",
    "\n",
    "    # Combine data\n",
    "    data = jnp.vstack([clean_data, outlier_data])\n",
    "\n",
    "    # Shuffle the data\n",
    "    perm = jax.random.permutation(keys[2], n_samples)\n",
    "    data = data[perm]\n",
    "\n",
    "    return data, true_cov\n",
    "\n",
    "# Generate test data\n",
    "key = jax.random.key(42)\n",
    "data, true_cov = generate_multivariate_data_with_outliers(key, n_samples=200, n_features=4, outlier_ratio=0.15)\n",
    "\n",
    "print(f\"Generated data shape: {data.shape}\")\n",
    "print(f\"Outlier ratio: 15%\")\n",
    "print(f\"\\nTrue covariance matrix:\")\n",
    "print(true_cov)\n",
    "print(f\"\\nData statistics:\")\n",
    "print(f\"Mean: {jnp.mean(data, axis=0)}\")\n",
    "print(f\"Std:  {jnp.std(data, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard vs Robust Estimation\n",
    "\n",
    "Let's compare traditional maximum likelihood estimation with robust manifold-based estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_covariance(data):\n",
    "    \"\"\"Standard maximum likelihood estimation of covariance matrix.\"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    centered_data = data - jnp.mean(data, axis=0)\n",
    "    return (centered_data.T @ centered_data) / (n_samples - 1)\n",
    "\n",
    "def robust_manifold_covariance_cost(cov_matrix, data, huber_delta=1.5):\n",
    "    \"\"\"\n",
    "    Robust covariance estimation cost function using Huber loss.\n",
    "\n",
    "    This cost function uses the Mahalanobis distance with Huber loss\n",
    "    to reduce the influence of outliers on covariance estimation.\n",
    "    \"\"\"\n",
    "    centered_data = data - jnp.mean(data, axis=0)\n",
    "\n",
    "    # Compute Mahalanobis distances\n",
    "    cov_inv = jnp.linalg.inv(cov_matrix)\n",
    "    mahalanobis_sq = jnp.sum((centered_data @ cov_inv) * centered_data, axis=1)\n",
    "\n",
    "    # Apply Huber loss to reduce outlier influence\n",
    "    def huber_loss(x, delta):\n",
    "        condition = jnp.abs(x) <= delta\n",
    "        quadratic = 0.5 * x**2\n",
    "        linear = delta * (jnp.abs(x) - 0.5 * delta)\n",
    "        return jnp.where(condition, quadratic, linear)\n",
    "\n",
    "    # Negative log-likelihood with Huber loss\n",
    "    log_det_term = jnp.log(jnp.linalg.det(cov_matrix))\n",
    "    huber_distances = jax.vmap(lambda x: huber_loss(jnp.sqrt(x), huber_delta))(mahalanobis_sq)\n",
    "\n",
    "    return log_det_term + jnp.mean(huber_distances)\n",
    "\n",
    "# Standard MLE estimation\n",
    "mle_cov = mle_covariance(data)\n",
    "print(\"Standard MLE covariance estimate:\")\n",
    "print(mle_cov)\n",
    "print(f\"\\nMLE Frobenius error: {jnp.linalg.norm(mle_cov - true_cov, 'fro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Estimation on SPD Manifold\n",
    "\n",
    "Now we perform robust covariance estimation using Riemannian optimization on the SPD manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_covariance_manifold(data, method=\"radam\", max_iterations=100):\n",
    "    \"\"\"Optimize covariance matrix on SPD manifold using robust estimation.\"\"\"\n",
    "    n_features = data.shape[1]\n",
    "    spd = rx.SymmetricPositiveDefinite(n=n_features)\n",
    "\n",
    "    # Define robust cost function\n",
    "    def cost_fn(C):\n",
    "        return robust_manifold_covariance_cost(C, data)\n",
    "\n",
    "    # Create optimization problem\n",
    "    problem = rx.RiemannianProblem(spd, cost_fn)\n",
    "\n",
    "    # Initialize with MLE as starting point\n",
    "    # Ensure it's positive definite by adding small regularization\n",
    "    initial_cov = mle_cov + 1e-6 * jnp.eye(n_features)\n",
    "\n",
    "    # Solve the optimization problem\n",
    "    result = rx.minimize(\n",
    "        problem,\n",
    "        initial_cov,\n",
    "        method=method,\n",
    "        options={\"learning_rate\": 0.01, \"max_iterations\": max_iterations}\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "# Perform robust estimation\n",
    "print(\"Optimizing robust covariance estimate on SPD manifold...\")\n",
    "robust_result = optimize_covariance_manifold(data)\n",
    "\n",
    "print(f\"\\nOptimization completed in {robust_result.niter} iterations\")\n",
    "print(f\"Final cost: {robust_result.fun:.6f}\")\n",
    "\n",
    "robust_cov = robust_result.x\n",
    "print(\"\\nRobust manifold covariance estimate:\")\n",
    "print(robust_cov)\n",
    "print(f\"\\nRobust Frobenius error: {jnp.linalg.norm(robust_cov - true_cov, 'fro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three covariance estimates\n",
    "print(\"=\" * 60)\n",
    "print(\"COVARIANCE ESTIMATION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute error metrics\n",
    "mle_error = jnp.linalg.norm(mle_cov - true_cov, 'fro')\n",
    "robust_error = jnp.linalg.norm(robust_cov - true_cov, 'fro')\n",
    "\n",
    "print(f\"\\nFrobenius Norm Errors:\")\n",
    "print(f\"MLE Error:       {mle_error:.4f}\")\n",
    "print(f\"Robust Error:    {robust_error:.4f}\")\n",
    "print(f\"Improvement:     {((mle_error - robust_error) / mle_error * 100):.1f}%\")\n",
    "\n",
    "# Compute condition numbers\n",
    "true_cond = jnp.linalg.cond(true_cov)\n",
    "mle_cond = jnp.linalg.cond(mle_cov)\n",
    "robust_cond = jnp.linalg.cond(robust_cov)\n",
    "\n",
    "print(f\"\\nCondition Numbers:\")\n",
    "print(f\"True:            {true_cond:.2f}\")\n",
    "print(f\"MLE:             {mle_cond:.2f}\")\n",
    "print(f\"Robust:          {robust_cond:.2f}\")\n",
    "\n",
    "# Check positive definiteness\n",
    "def check_spd(matrix, name):\n",
    "    eigenvals = jnp.linalg.eigvals(matrix)\n",
    "    min_eigval = jnp.min(eigenvals)\n",
    "    print(f\"{name:12} min eigenvalue: {min_eigval:.6f} {'✓' if min_eigval > 0 else '✗'}\")\n",
    "\n",
    "print(f\"\\nPositive Definiteness Check:\")\n",
    "check_spd(true_cov, \"True\")\n",
    "check_spd(mle_cov, \"MLE\")\n",
    "check_spd(robust_cov, \"Robust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_covariance_comparison(true_cov, mle_cov, robust_cov):\n",
    "    \"\"\"Plot comparison of covariance matrices.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Covariance matrices heatmaps\n",
    "    matrices = [true_cov, mle_cov, robust_cov]\n",
    "    titles = [\"True Covariance\", \"MLE Estimate\", \"Robust Estimate\"]\n",
    "    \n",
    "    vmin = min(mat.min() for mat in matrices)\n",
    "    vmax = max(mat.max() for mat in matrices)\n",
    "    \n",
    "    for i, (mat, title) in enumerate(zip(matrices, titles)):\n",
    "        im = axes[0, i].imshow(mat, cmap='RdBu_r', vmin=vmin, vmax=vmax)\n",
    "        axes[0, i].set_title(title)\n",
    "        axes[0, i].set_xlabel('Feature')\n",
    "        axes[0, i].set_ylabel('Feature')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[0, i], shrink=0.8)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for j in range(mat.shape[0]):\n",
    "            for k in range(mat.shape[1]):\n",
    "                text = axes[0, i].text(k, j, f'{mat[j, k]:.2f}', \n",
    "                                     ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "    \n",
    "    # Error matrices\n",
    "    mle_error_mat = jnp.abs(mle_cov - true_cov)\n",
    "    robust_error_mat = jnp.abs(robust_cov - true_cov)\n",
    "    \n",
    "    error_matrices = [mle_error_mat, robust_error_mat]\n",
    "    error_titles = [\"MLE Error (|est - true|)\", \"Robust Error (|est - true|)\"]\n",
    "    \n",
    "    error_vmax = max(mat.max() for mat in error_matrices)\n",
    "    \n",
    "    for i, (mat, title) in enumerate(zip(error_matrices, error_titles)):\n",
    "        im = axes[1, i].imshow(mat, cmap='Reds', vmin=0, vmax=error_vmax)\n",
    "        axes[1, i].set_title(title)\n",
    "        axes[1, i].set_xlabel('Feature')\n",
    "        axes[1, i].set_ylabel('Feature')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[1, i], shrink=0.8)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for j in range(mat.shape[0]):\n",
    "            for k in range(mat.shape[1]):\n",
    "                text = axes[1, i].text(k, j, f'{mat[j, k]:.3f}', \n",
    "                                     ha=\"center\", va=\"center\", color=\"white\", fontsize=10)\n",
    "    \n",
    "    # Bar plot comparison\n",
    "    methods = ['MLE', 'Robust']\n",
    "    errors = [mle_error, robust_error]\n",
    "    colors = ['skyblue', 'lightcoral']\n",
    "    \n",
    "    bars = axes[1, 2].bar(methods, errors, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 2].set_title('Frobenius Norm Error Comparison')\n",
    "    axes[1, 2].set_ylabel('||Estimate - True||_F')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, error in zip(bars, errors):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + error*0.01,\n",
    "                        f'{error:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create comparison visualization\n",
    "fig = plot_covariance_comparison(true_cov, mle_cov, robust_cov)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_with_ellipses(data, true_cov, mle_cov, robust_cov):\n",
    "    \"\"\"Plot data points with confidence ellipses for different covariance estimates.\"\"\"\n",
    "    # Plot first two dimensions for visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    data_2d = data[:, :2]\n",
    "    mean_2d = jnp.mean(data_2d, axis=0)\n",
    "    \n",
    "    covariances = [true_cov[:2, :2], mle_cov[:2, :2], robust_cov[:2, :2]]\n",
    "    titles = ['True Covariance', 'MLE Estimate', 'Robust Estimate']\n",
    "    colors = ['green', 'blue', 'red']\n",
    "    \n",
    "    for i, (cov_2d, title, color) in enumerate(zip(covariances, titles, colors)):\n",
    "        # Scatter plot of data\n",
    "        axes[i].scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6, s=30, color='gray')\n",
    "        \n",
    "        # Plot confidence ellipses\n",
    "        from matplotlib.patches import Ellipse\n",
    "        \n",
    "        for confidence in [0.5, 0.95]:  # 50% and 95% confidence\n",
    "            # Chi-square quantile for 2D\n",
    "            chi2_val = -2 * np.log(1 - confidence)\n",
    "            \n",
    "            # Eigendecomposition for ellipse orientation\n",
    "            eigenvals, eigenvecs = jnp.linalg.eigh(cov_2d)\n",
    "            \n",
    "            # Ellipse parameters\n",
    "            angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))\n",
    "            width = 2 * np.sqrt(chi2_val * eigenvals[0])\n",
    "            height = 2 * np.sqrt(chi2_val * eigenvals[1])\n",
    "            \n",
    "            # Create ellipse\n",
    "            alpha = 0.3 if confidence == 0.95 else 0.6\n",
    "            ellipse = Ellipse(mean_2d, width, height, angle=angle, \n",
    "                            facecolor=color, alpha=alpha, edgecolor=color, linewidth=2)\n",
    "            axes[i].add_patch(ellipse)\n",
    "        \n",
    "        axes[i].set_title(f'{title}\\nConfidence Ellipses (50% & 95%)')\n",
    "        axes[i].set_xlabel('Feature 1')\n",
    "        axes[i].set_ylabel('Feature 2')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create ellipse visualization\n",
    "ellipse_fig = plot_data_with_ellipses(data, true_cov, mle_cov, robust_cov)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold Properties Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze SPD manifold properties\n",
    "n_features = data.shape[1]\n",
    "spd = rx.SymmetricPositiveDefinite(n=n_features)\n",
    "\n",
    "print(\"SPD Manifold Properties:\")\n",
    "print(f\"Matrix dimension: {n_features}x{n_features}\")\n",
    "print(f\"Manifold dimension: {spd.dimension}\")\n",
    "print(f\"Ambient space dimension: {spd.ambient_dimension}\")\n",
    "\n",
    "# Test geodesic distance between estimates\n",
    "mle_robust_dist = spd.dist(mle_cov, robust_cov)\n",
    "true_mle_dist = spd.dist(true_cov, mle_cov)\n",
    "true_robust_dist = spd.dist(true_cov, robust_cov)\n",
    "\n",
    "print(f\"\\nGeodesic Distances on SPD Manifold:\")\n",
    "print(f\"True ↔ MLE:       {true_mle_dist:.4f}\")\n",
    "print(f\"True ↔ Robust:    {true_robust_dist:.4f}\")\n",
    "print(f\"MLE ↔ Robust:     {mle_robust_dist:.4f}\")\n",
    "\n",
    "# Test tangent space properties\n",
    "random_tangent = jax.random.normal(jax.random.key(999), (n_features, n_features))\n",
    "# Make it symmetric (tangent vectors to SPD are symmetric matrices)\n",
    "random_tangent = (random_tangent + random_tangent.T) / 2\n",
    "\n",
    "projected_tangent = spd.proj(robust_cov, random_tangent)\n",
    "print(f\"\\nTangent space projection test:\")\n",
    "print(f\"Original tangent symmetry error: {jnp.linalg.norm(random_tangent - random_tangent.T):.2e}\")\n",
    "print(f\"Projected tangent symmetry error: {jnp.linalg.norm(projected_tangent - projected_tangent.T):.2e}\")\n",
    "\n",
    "print(\"\\nRobust SPD covariance estimation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **SPD Manifold Structure**: Working with symmetric positive definite matrices as a Riemannian manifold\n",
    "2. **Robust Estimation**: Using Huber loss to reduce outlier influence in covariance estimation\n",
    "3. **Manifold Optimization**: Leveraging Riemannian optimization to respect the geometric constraints\n",
    "4. **Comparison Analysis**: Quantitative and visual comparison of different estimation methods\n",
    "5. **Geometric Properties**: Understanding geodesic distances and tangent spaces on the SPD manifold\n",
    "\n",
    "The results typically show that robust manifold-based estimation provides better accuracy when outliers are present, while maintaining the positive definite constraint naturally through the manifold structure.\n",
    "\n",
    "Key advantages of the manifold approach:\n",
    "- **Geometric Consistency**: Respects the natural geometry of positive definite matrices\n",
    "- **Constraint Satisfaction**: Automatically maintains positive definiteness\n",
    "- **Outlier Robustness**: Huber loss reduces influence of extreme values\n",
    "- **Theoretical Foundation**: Based on well-established Riemannian geometry principles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
