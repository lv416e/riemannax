{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61b3d45",
   "metadata": {},
   "source": [
    "# Advanced Riemannian Optimization with RiemannAX\n",
    "\n",
    "This notebook provides an in-depth exploration of advanced Riemannian optimization concepts and their implementation in RiemannAX. We'll cover:\n",
    "\n",
    "1. **Mathematical Foundations**: Riemannian geometry basics\n",
    "2. **Manifold Operations**: Deep dive into exponential maps, parallel transport\n",
    "3. **Optimization Algorithms**: Comparative analysis of SGD, Adam, and Momentum\n",
    "4. **Numerical Considerations**: Stability, conditioning, and performance\n",
    "5. **Advanced Applications**: Multi-manifold optimization and custom problems\n",
    "\n",
    "**Prerequisites**: Linear algebra, basic optimization theory, Python/JAX familiarity\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand when and why to use Riemannian optimization\n",
    "- Master the RiemannAX API for complex optimization problems\n",
    "- Develop intuition for manifold geometry and optimization dynamics\n",
    "- Learn best practices for numerical stability and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb82c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# RiemannAX imports\n",
    "import riemannax as rx\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "# JAX configuration\n",
    "jax.config.update(\"jax_enable_x64\", True)  # Use double precision\n",
    "\n",
    "print(\"RiemannAX Advanced Tutorial - Setup Complete\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b4de4",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundations\n",
    "\n",
    "### Riemannian Manifolds: Key Concepts\n",
    "\n",
    "A **Riemannian manifold** $(M, g)$ is a smooth manifold $M$ equipped with a Riemannian metric $g$ that defines:\n",
    "\n",
    "- **Tangent Spaces**: $T_x M$ at each point $x \\in M$\n",
    "- **Inner Products**: $g_x : T_x M \\times T_x M \\to \\mathbb{R}$\n",
    "- **Geodesics**: Shortest paths on the manifold\n",
    "- **Exponential Map**: $\\exp_x : T_x M \\to M$\n",
    "- **Parallel Transport**: Moving tangent vectors along curves\n",
    "\n",
    "### Why Riemannian Optimization?\n",
    "\n",
    "Many optimization problems have **natural constraints** that define manifold structure:\n",
    "- Orthogonality constraints → Stiefel/Grassmann manifolds\n",
    "- Positive definiteness → SPD manifolds  \n",
    "- Unit norm constraints → Sphere manifolds\n",
    "\n",
    "Riemannian optimization respects these constraints **exactly** at every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore manifold operations with the sphere\n",
    "sphere = rx.Sphere()\n",
    "\n",
    "# Generate random points on the sphere\n",
    "key = jax.random.key(42)\n",
    "keys = jax.random.split(key, 5)\n",
    "\n",
    "x = sphere.random_point(keys[0])\n",
    "y = sphere.random_point(keys[1])\n",
    "\n",
    "print(\"Sphere Manifold Operations Demo\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Point x: {x}\")\n",
    "print(f\"Point y: {y}\")\n",
    "print(f\"||x|| = {jnp.linalg.norm(x):.6f} (should be 1.0)\")\n",
    "print(f\"||y|| = {jnp.linalg.norm(y):.6f} (should be 1.0)\")\n",
    "\n",
    "# Logarithmic map: find tangent vector from x to y\n",
    "log_xy = sphere.log(x, y)\n",
    "print(f\"\\nLogarithmic map log_x(y): {log_xy}\")\n",
    "print(f\"||log_x(y)|| = {jnp.linalg.norm(log_xy):.6f} (geodesic distance)\")\n",
    "\n",
    "# Verify it's in tangent space (orthogonal to x)\n",
    "print(f\"<x, log_x(y)> = {jnp.dot(x, log_xy):.10f} (should be ~0)\")\n",
    "\n",
    "# Exponential map: recover y from x and tangent vector\n",
    "exp_result = sphere.exp(x, log_xy)\n",
    "print(f\"\\nExponential map exp_x(log_x(y)): {exp_result}\")\n",
    "print(f\"||y - exp_x(log_x(y))|| = {jnp.linalg.norm(y - exp_result):.10f}\")\n",
    "\n",
    "# Geodesic distance\n",
    "distance = sphere.dist(x, y)\n",
    "print(f\"\\nGeodesic distance d(x,y) = {distance:.6f}\")\n",
    "print(f\"Euclidean distance ||x-y|| = {jnp.linalg.norm(x - y):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3a8a2",
   "metadata": {},
   "source": [
    "## 2. Deep Dive: Manifold Operations\n",
    "\n",
    "### Parallel Transport\n",
    "\n",
    "Parallel transport is crucial for Riemannian optimization algorithms like Adam and Momentum. It allows us to \"move\" tangent vectors from one point to another while preserving their geometric properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_parallel_transport():\n",
    "    \"\"\"Visualize parallel transport on the sphere.\"\"\"\n",
    "    sphere = rx.Sphere()\n",
    "\n",
    "    # Create a geodesic path\n",
    "    key = jax.random.key(123)\n",
    "    keys = jax.random.split(key, 3)\n",
    "\n",
    "    x = sphere.random_point(keys[0])\n",
    "    y = sphere.random_point(keys[1])\n",
    "    v = sphere.random_tangent(keys[2], x)\n",
    "\n",
    "    # Normalize tangent vector for visualization\n",
    "    v = 0.3 * v / jnp.linalg.norm(v)\n",
    "\n",
    "    print(\"Parallel Transport Demonstration\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Starting point x: {x}\")\n",
    "    print(f\"Ending point y: {y}\")\n",
    "    print(f\"Tangent vector v at x: {v}\")\n",
    "    print(f\"||v|| = {jnp.linalg.norm(v):.6f}\")\n",
    "\n",
    "    # Parallel transport v from x to y\n",
    "    v_transported = sphere.transp(x, y, v)\n",
    "\n",
    "    print(f\"\\nTransported vector at y: {v_transported}\")\n",
    "    print(f\"||v_transported|| = {jnp.linalg.norm(v_transported):.6f}\")\n",
    "\n",
    "    # Verify transported vector is in tangent space at y\n",
    "    print(f\"<y, v_transported> = {jnp.dot(y, v_transported):.10f} (should be ~0)\")\n",
    "\n",
    "    # Verify norm preservation (key property of parallel transport)\n",
    "    norm_preserved = jnp.allclose(jnp.linalg.norm(v), jnp.linalg.norm(v_transported))\n",
    "    print(f\"Norm preserved: {norm_preserved}\")\n",
    "\n",
    "    return x, y, v, v_transported\n",
    "\n",
    "\n",
    "x, y, v, v_transported = visualize_parallel_transport()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72c3dd",
   "metadata": {},
   "source": [
    "## 3. Optimization Algorithm Deep Dive\n",
    "\n",
    "### Riemannian SGD\n",
    "$$x_{k+1} = \\text{Exp}_{x_k}(-\\alpha \\text{grad} f(x_k))$$\n",
    "\n",
    "### Riemannian Adam\n",
    "$$m_{k+1} = \\beta_1 \\mathcal{T}_{x_k \\to x_{k+1}} m_k + (1-\\beta_1) \\text{grad} f(x_k)$$\n",
    "$$v_{k+1} = \\beta_2 \\mathcal{T}_{x_k \\to x_{k+1}} v_k + (1-\\beta_2) (\\text{grad} f(x_k))^2$$\n",
    "\n",
    "where $\\mathcal{T}_{x \\to y}$ denotes parallel transport.\n",
    "\n",
    "### Riemannian Momentum\n",
    "$$m_{k+1} = \\mu \\mathcal{T}_{x_k \\to x_{k+1}} m_k + \\text{grad} f(x_k)$$\n",
    "$$x_{k+1} = \\text{Exp}_{x_k}(-\\alpha m_{k+1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_optimizer_analysis():\n",
    "    \"\"\"Detailed analysis of optimizer behavior.\"\"\"\n",
    "    # Create a challenging optimization problem on SO(3)\n",
    "    so3 = rx.SpecialOrthogonal(n=3)\n",
    "\n",
    "    # Target: specific rotation matrix\n",
    "    key = jax.random.key(456)\n",
    "    keys = jax.random.split(key, 3)\n",
    "\n",
    "    target = so3.random_point(keys[0])\n",
    "    x0 = so3.random_point(keys[1])\n",
    "\n",
    "    def cost_fn(R):\n",
    "        # Geodesic distance squared (more challenging than Frobenius norm)\n",
    "        return 0.5 * so3.dist(R, target) ** 2\n",
    "\n",
    "    problem = rx.RiemannianProblem(so3, cost_fn)\n",
    "\n",
    "    print(\"Detailed Optimizer Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Problem: SO(3) rotation alignment\")\n",
    "    print(f\"Initial cost: {cost_fn(x0):.6f}\")\n",
    "    print(\"Target cost: 0.0\")\n",
    "\n",
    "    # Test different optimizers with manual stepping\n",
    "    max_iterations = 100\n",
    "\n",
    "    optimizers = {\n",
    "        \"RSGD\": rx.riemannian_gradient_descent(learning_rate=0.1),\n",
    "        \"RAdaM\": rx.riemannian_adam(learning_rate=0.01, beta1=0.9, beta2=0.999),\n",
    "        \"RMomentum\": rx.riemannian_momentum(learning_rate=0.05, momentum=0.9),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, (init_fn, update_fn) in optimizers.items():\n",
    "        print(f\"\\nAnalyzing {name}...\")\n",
    "\n",
    "        state = init_fn(x0)\n",
    "        costs = [float(cost_fn(state.x))]\n",
    "        grad_norms = []\n",
    "        step_sizes = []\n",
    "\n",
    "        for _i in range(max_iterations):\n",
    "            # Compute gradient\n",
    "            gradient = problem.grad(state.x)\n",
    "            grad_norm = jnp.linalg.norm(gradient)\n",
    "            grad_norms.append(float(grad_norm))\n",
    "\n",
    "            # Store previous state for step size analysis\n",
    "            prev_x = state.x\n",
    "\n",
    "            # Update\n",
    "            state = update_fn(gradient, state, so3)\n",
    "\n",
    "            # Compute step size\n",
    "            step_size = so3.dist(prev_x, state.x)\n",
    "            step_sizes.append(float(step_size))\n",
    "\n",
    "            # Record cost\n",
    "            current_cost = float(cost_fn(state.x))\n",
    "            costs.append(current_cost)\n",
    "\n",
    "            # Early stopping\n",
    "            if current_cost < 1e-10:\n",
    "                break\n",
    "\n",
    "        results[name] = {\n",
    "            \"costs\": costs,\n",
    "            \"grad_norms\": grad_norms,\n",
    "            \"step_sizes\": step_sizes,\n",
    "            \"final_cost\": costs[-1],\n",
    "            \"iterations\": len(costs) - 1,\n",
    "        }\n",
    "\n",
    "        print(f\"  Final cost: {costs[-1]:.2e}\")\n",
    "        print(f\"  Iterations: {len(costs) - 1}\")\n",
    "        print(f\"  Final gradient norm: {grad_norms[-1]:.2e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run detailed analysis\n",
    "optimizer_results = detailed_optimizer_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optimizer_analysis(results):\n",
    "    \"\"\"Create detailed plots of optimizer behavior.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    colors = {\"RSGD\": \"red\", \"RAdaM\": \"blue\", \"RMomentum\": \"green\"}\n",
    "\n",
    "    # 1. Convergence plot\n",
    "    ax = axes[0, 0]\n",
    "    for name, data in results.items():\n",
    "        iterations = range(len(data[\"costs\"]))\n",
    "        ax.semilogy(iterations, data[\"costs\"], color=colors[name], label=name, linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title(\"Convergence Comparison\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Cost (log scale)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Gradient norm evolution\n",
    "    ax = axes[0, 1]\n",
    "    for name, data in results.items():\n",
    "        iterations = range(len(data[\"grad_norms\"]))\n",
    "        ax.semilogy(iterations, data[\"grad_norms\"], color=colors[name], label=name, linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title(\"Gradient Norm Evolution\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"||grad f|| (log scale)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Step size evolution\n",
    "    ax = axes[1, 0]\n",
    "    for name, data in results.items():\n",
    "        iterations = range(len(data[\"step_sizes\"]))\n",
    "        ax.plot(iterations, data[\"step_sizes\"], color=colors[name], label=name, linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_title(\"Step Size Evolution\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Step Size (geodesic distance)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Performance summary\n",
    "    ax = axes[1, 1]\n",
    "\n",
    "    names = list(results.keys())\n",
    "    final_costs = [results[name][\"final_cost\"] for name in names]\n",
    "    iterations = [results[name][\"iterations\"] for name in names]\n",
    "\n",
    "    x_pos = np.arange(len(names))\n",
    "\n",
    "    # Dual y-axis plot\n",
    "    bars1 = ax.bar(x_pos - 0.2, final_costs, 0.4, color=[colors[name] for name in names], alpha=0.7, label=\"Final Cost\")\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    bars2 = ax2.bar(x_pos + 0.2, iterations, 0.4, color=\"gray\", alpha=0.5, label=\"Iterations\")\n",
    "\n",
    "    ax.set_xlabel(\"Optimizer\")\n",
    "    ax.set_ylabel(\"Final Cost\", color=\"black\")\n",
    "    ax2.set_ylabel(\"Iterations\", color=\"gray\")\n",
    "    ax.set_title(\"Final Performance Summary\")\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, cost in zip(bars1, final_costs, strict=False):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2, bar.get_height(), f\"{cost:.1e}\", ha=\"center\", va=\"bottom\", fontsize=9\n",
    "        )\n",
    "\n",
    "    for bar, iters in zip(bars2, iterations, strict=False):\n",
    "        ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f\"{iters}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    output_dir = Path(\"./output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    plt.savefig(output_dir / \"advanced_optimizer_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create detailed plots\n",
    "plot_optimizer_analysis(optimizer_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3bbf1",
   "metadata": {},
   "source": [
    "## 4. Numerical Considerations\n",
    "\n",
    "### Conditioning and Stability\n",
    "\n",
    "Riemannian optimization can face unique numerical challenges:\n",
    "\n",
    "1. **Exponential Map Computation**: Can be expensive for some manifolds\n",
    "2. **Parallel Transport**: Numerical errors can accumulate\n",
    "3. **Manifold Constraints**: Must be preserved exactly\n",
    "4. **Step Size Selection**: Too large steps can leave the manifold\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use **retraction** when exponential map is expensive\n",
    "- **Clip step sizes** to prevent overshooting\n",
    "- **Validate manifold constraints** regularly\n",
    "- Consider **double precision** for ill-conditioned problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_stability_analysis():\n",
    "    \"\"\"Analyze numerical stability across different manifolds.\"\"\"\n",
    "    print(\"Numerical Stability Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Test different manifolds with varying conditioning\n",
    "    manifolds = {\n",
    "        \"Sphere\": rx.Sphere(),\n",
    "        \"SO(3)\": rx.SpecialOrthogonal(n=3),\n",
    "        \"Grassmann(4,2)\": rx.Grassmann(n=4, p=2),\n",
    "        \"SPD(3)\": rx.SymmetricPositiveDefinite(n=3),\n",
    "    }\n",
    "\n",
    "    key = jax.random.key(789)\n",
    "\n",
    "    for name, manifold in manifolds.items():\n",
    "        print(f\"\\nTesting {name}:\")\n",
    "\n",
    "        # Generate random point\n",
    "        key, subkey = jax.random.split(key)\n",
    "        x = manifold.random_point(subkey)\n",
    "\n",
    "        # Test manifold constraint satisfaction\n",
    "        if hasattr(manifold, \"_is_in_manifold\"):\n",
    "            on_manifold = manifold._is_in_manifold(x)\n",
    "            print(f\"  Point on manifold: {on_manifold}\")\n",
    "\n",
    "        # Test exp/log consistency\n",
    "        key, subkey = jax.random.split(key)\n",
    "        v = manifold.random_tangent(subkey, x)\n",
    "\n",
    "        # Scale tangent vector to test different step sizes\n",
    "        for scale in [0.1, 0.5, 1.0]:\n",
    "            v_scaled = scale * v\n",
    "\n",
    "            try:\n",
    "                # Test exp/log round trip\n",
    "                y = manifold.exp(x, v_scaled)\n",
    "                v_recovered = manifold.log(x, y)\n",
    "\n",
    "                error = jnp.linalg.norm(v_scaled - v_recovered)\n",
    "                print(f\"  Scale {scale}: exp/log error = {error:.2e}\")\n",
    "\n",
    "                # Test if y is on manifold\n",
    "                if hasattr(manifold, \"_is_in_manifold\"):\n",
    "                    y_on_manifold = manifold._is_in_manifold(y)\n",
    "                    if not y_on_manifold:\n",
    "                        print(\"    WARNING: exp result not on manifold!\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Scale {scale}: ERROR - {e!s}\")\n",
    "\n",
    "        # Test parallel transport properties\n",
    "        key, subkey = jax.random.split(key)\n",
    "        y = manifold.random_point(subkey)\n",
    "\n",
    "        v_transported = manifold.transp(x, y, v)\n",
    "\n",
    "        # Check norm preservation\n",
    "        norm_error = abs(jnp.linalg.norm(v) - jnp.linalg.norm(v_transported))\n",
    "        print(f\"  Parallel transport norm error: {norm_error:.2e}\")\n",
    "\n",
    "        # Check tangent space property\n",
    "        if hasattr(manifold, \"proj\"):\n",
    "            v_projected = manifold.proj(y, v_transported)\n",
    "            tangent_error = jnp.linalg.norm(v_transported - v_projected)\n",
    "            print(f\"  Tangent space error: {tangent_error:.2e}\")\n",
    "\n",
    "\n",
    "numerical_stability_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7e4b9",
   "metadata": {},
   "source": [
    "## 5. Advanced Applications\n",
    "\n",
    "### Multi-Manifold Optimization\n",
    "\n",
    "Some problems involve optimization over **multiple manifolds simultaneously**. For example:\n",
    "- Joint diagonalization: $\\text{SO}(n) \\times \\text{SPD}(n)$\n",
    "- Subspace clustering: Multiple Grassmann manifolds\n",
    "- Multi-view learning: Product of Stiefel manifolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c96712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_diagonalization_demo():\n",
    "    \"\"\"Demonstrate joint diagonalization using SO(n) x SPD(n).\"\"\"\n",
    "    print(\"Joint Diagonalization Demo\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Problem: Given matrices A1, A2, ..., find rotation R such that\n",
    "    # R^T A_i R are as diagonal as possible\n",
    "\n",
    "    key = jax.random.key(999)\n",
    "    keys = jax.random.split(key, 10)\n",
    "\n",
    "    n = 4\n",
    "    n_matrices = 3\n",
    "\n",
    "    # Generate test matrices with known joint diagonalizer\n",
    "    true_rotation = rx.SpecialOrthogonal(n).random_point(keys[0])\n",
    "\n",
    "    # Create matrices that are diagonalized by true_rotation\n",
    "    matrices = []\n",
    "    for i in range(n_matrices):\n",
    "        # Random diagonal matrix\n",
    "        D = jnp.diag(jax.random.uniform(keys[i + 1], (n,), minval=0.1, maxval=2.0))\n",
    "        # Rotate to create non-diagonal matrix\n",
    "        A = true_rotation @ D @ true_rotation.T\n",
    "        matrices.append(A)\n",
    "\n",
    "    print(f\"Generated {n_matrices} matrices of size {n}x{n}\")\n",
    "    print(\"True rotation matrix known\")\n",
    "\n",
    "    # Define joint diagonalization cost\n",
    "    def joint_diag_cost(R):\n",
    "        total_off_diag = 0.0\n",
    "        for A in matrices:\n",
    "            # Transform matrix\n",
    "            transformed = R.T @ A @ R\n",
    "            # Penalize off-diagonal elements\n",
    "            off_diag_mask = 1 - jnp.eye(n)\n",
    "            off_diag_sum = jnp.sum((transformed * off_diag_mask) ** 2)\n",
    "            total_off_diag += off_diag_sum\n",
    "        return total_off_diag\n",
    "\n",
    "    # Initial cost\n",
    "    initial_cost = joint_diag_cost(jnp.eye(n))\n",
    "    true_cost = joint_diag_cost(true_rotation)\n",
    "\n",
    "    print(f\"\\nCost with identity: {initial_cost:.6f}\")\n",
    "    print(f\"Cost with true rotation: {true_cost:.6f}\")\n",
    "\n",
    "    # Optimize using different methods\n",
    "    so3 = rx.SpecialOrthogonal(n)\n",
    "    problem = rx.RiemannianProblem(so3, joint_diag_cost)\n",
    "\n",
    "    # Random initialization\n",
    "    x0 = so3.random_point(keys[-1])\n",
    "    initial_opt_cost = joint_diag_cost(x0)\n",
    "\n",
    "    print(f\"Initial optimization cost: {initial_opt_cost:.6f}\")\n",
    "\n",
    "    # Test different optimizers\n",
    "    methods = [\"rsgd\", \"radam\", \"rmom\"]\n",
    "    results = {}\n",
    "\n",
    "    for method in methods:\n",
    "        print(f\"\\nOptimizing with {method.upper()}...\")\n",
    "\n",
    "        if method == \"radam\":\n",
    "            options = {\"learning_rate\": 0.01, \"max_iterations\": 200}\n",
    "        elif method == \"rmom\":\n",
    "            options = {\"learning_rate\": 0.05, \"momentum\": 0.9, \"max_iterations\": 200}\n",
    "        else:\n",
    "            options = {\"learning_rate\": 0.1, \"max_iterations\": 200}\n",
    "\n",
    "        result = rx.minimize(problem, x0, method=method, options=options)\n",
    "\n",
    "        final_cost = result.fun\n",
    "        optimal_R = result.x\n",
    "\n",
    "        # Measure distance to true solution\n",
    "        rotation_error = so3.dist(optimal_R, true_rotation)\n",
    "\n",
    "        results[method] = {\"final_cost\": final_cost, \"rotation_error\": rotation_error, \"optimal_R\": optimal_R}\n",
    "\n",
    "        print(f\"  Final cost: {final_cost:.6f}\")\n",
    "        print(f\"  Distance to true rotation: {rotation_error:.6f}\")\n",
    "        print(f\"  Improvement: {((initial_opt_cost - final_cost) / initial_opt_cost * 100):.2f}%\")\n",
    "\n",
    "    return results, matrices, true_rotation\n",
    "\n",
    "\n",
    "joint_diag_results, matrices, true_rotation = joint_diagonalization_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35b0e5",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization and Best Practices\n",
    "\n",
    "### JAX Optimization Tips\n",
    "\n",
    "1. **JIT Compilation**: Use `@jax.jit` for expensive operations\n",
    "2. **Vectorization**: Use `jax.vmap` for batch operations\n",
    "3. **Memory Management**: Be aware of JAX's functional programming model\n",
    "4. **Device Placement**: Utilize GPU/TPU when available\n",
    "\n",
    "### RiemannAX Specific Tips\n",
    "\n",
    "1. **Choose the Right Manifold**: Match problem structure to manifold geometry\n",
    "2. **Optimizer Selection**: Adam for ill-conditioned, Momentum for speed, SGD for simplicity\n",
    "3. **Learning Rate Tuning**: Start conservative, especially for Adam\n",
    "4. **Retraction vs Exponential**: Use retraction for computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_comparison_demo():\n",
    "    \"\"\"Compare performance of different implementation choices.\"\"\"\n",
    "    print(\"Performance Comparison Demo\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Large-scale Grassmann optimization\n",
    "    n, p = 100, 10  # Large problem size\n",
    "    grassmann = rx.Grassmann(n=n, p=p)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    key = jax.random.key(12345)\n",
    "    keys = jax.random.split(key, 5)\n",
    "\n",
    "    # Large data matrix\n",
    "    data = jax.random.normal(keys[0], (n, 1000))\n",
    "\n",
    "    def subspace_cost(subspace):\n",
    "        projector = subspace @ subspace.T\n",
    "        reconstruction = projector @ data\n",
    "        return jnp.sum((data - reconstruction) ** 2)\n",
    "\n",
    "    problem = rx.RiemannianProblem(grassmann, subspace_cost)\n",
    "    x0 = grassmann.random_point(keys[1])\n",
    "\n",
    "    print(f\"Problem size: Grassmann({n}, {p})\")\n",
    "    print(f\"Data size: {data.shape}\")\n",
    "    print(f\"Initial cost: {subspace_cost(x0):.2e}\")\n",
    "\n",
    "    # Compare retraction vs exponential map\n",
    "    configurations = [(\"Exponential Map\", False), (\"Retraction\", True)]\n",
    "\n",
    "    for config_name, use_retraction in configurations:\n",
    "        print(f\"\\nTesting {config_name}:\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        result = rx.minimize(\n",
    "            problem,\n",
    "            x0,\n",
    "            method=\"rsgd\",\n",
    "            options={\"learning_rate\": 0.01, \"max_iterations\": 50, \"use_retraction\": use_retraction},\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(f\"  Time: {elapsed_time:.3f} seconds\")\n",
    "        print(f\"  Final cost: {result.fun:.2e}\")\n",
    "        print(f\"  Iterations: {getattr(result, 'nit', 'N/A')}\")\n",
    "\n",
    "    # Test batch optimization\n",
    "    print(\"\\nBatch Optimization Test:\")\n",
    "\n",
    "    batch_size = 5\n",
    "    batch_x0 = jax.vmap(lambda k: grassmann.random_point(k))(jax.random.split(keys[2], batch_size))\n",
    "\n",
    "    def batch_cost(batch_subspaces):\n",
    "        return jax.vmap(subspace_cost)(batch_subspaces)\n",
    "\n",
    "    # Time batch operation\n",
    "    start_time = time.time()\n",
    "    batch_costs = batch_cost(batch_x0)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Batch evaluation time: {elapsed_time:.4f} seconds\")\n",
    "    print(f\"  Average cost: {jnp.mean(batch_costs):.2e}\")\n",
    "\n",
    "\n",
    "performance_comparison_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf6fb17",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Geometric Insight**: Riemannian optimization provides natural solutions for constrained problems\n",
    "2. **Algorithm Choice**: Different optimizers excel in different scenarios\n",
    "3. **Numerical Care**: Stability and efficiency require thoughtful implementation\n",
    "4. **Practical Impact**: Real performance gains in machine learning applications\n",
    "\n",
    "### Advanced Topics to Explore\n",
    "\n",
    "- **Riemannian Natural Gradients**: Second-order methods\n",
    "- **Constrained Optimization**: Equality and inequality constraints on manifolds\n",
    "- **Stochastic Methods**: Riemannian SGD with mini-batches\n",
    "- **Multi-scale Optimization**: Hierarchical manifold optimization\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Absil, P.-A., Mahony, R., & Sepulchre, R. (2008). *Optimization Algorithms on Matrix Manifolds*\n",
    "- Boumal, N. (2023). *An Introduction to Optimization on Smooth Manifolds*\n",
    "- RiemannAX Documentation: [github.com/lv416e/riemannax](https://github.com/lv416e/riemannax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ac7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and cleanup\n",
    "print(\"=\" * 60)\n",
    "print(\"ADVANCED RIEMANNIAN OPTIMIZATION TUTORIAL COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✓ Mathematical foundations covered\")\n",
    "print(\"✓ Manifold operations understood\")\n",
    "print(\"✓ Optimization algorithms analyzed\")\n",
    "print(\"✓ Numerical considerations addressed\")\n",
    "print(\"✓ Advanced applications demonstrated\")\n",
    "print(\"✓ Performance optimization explored\")\n",
    "print(\"\\nNext: Apply these concepts to your own optimization problems!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riemannax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
